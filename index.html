
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-65957785-1', 'auto');
  ga('send', 'pageview');

</script>

<!-- Copied from wikipedia: https://en.wikipedia.org/wiki/Blink_element#Implementation -->
<script type="text/javascript">
  (function() {
    var blinks = document.getElementsByTagName('blink');
    var visibility = 'hidden';
    window.setInterval(function() {
      for (var i = blinks.length - 1; i >= 0; i--) {
        blinks[i].style.visibility = visibility;
      }
      visibility = (visibility === 'visible') ? 'hidden' : 'visible';
    }, 250);
  })();
</script>

<script type="text/javascript">
function toggle(obj) {
var obj=document.getElementById(obj);
if (obj.style.display == "block") obj.style.display = "none";
else obj.style.display = "block";
}
</script>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="style.css" />

<title>  Vivek Kumar</title>

</head>

<body>
	<style>
	div.floating-menu {position:fixed;background: #26476E url(background.png);border:1px ;width:220px;z-index:100;}
	div.floating-menu a, div.floating-menu h3 {display:block;margin:0 1.2em;}
	</style>
	<div class="floating-menu">
	<br>
	<br>
        <h1><span style="color: #008000"><a href="#photo"> Vivek Kumar</a></span></h1>
	<br>
	<h3>
        <a href="#about">Biography</a>
	<br>
        <a href="#news">News</a>
	<br>
<!--
        <a href="#research">Research</a>
	<br>
-->
        <a href="#publications">Publications</a>
	<br>
        <a href="#talks">Talks</a>
	<br>
        <a href="#teaching">Teaching</a>
	<br>
        <a href="#students">Students</a>
	<br>
        <a href="#awards">Awards</a>
	<br>
        <a href="#software">Software</a>
	<br>
        <a href="#services">Services</a>
	<br>
        <a href="#affiliations">Past Affiliations</a>
	<br>
        <a href="#photos">Photos</a>
	<br>
        <a href="#contact">Contact</a>
	</h3>
	</div>

	<div id="page">
	<br>
	<br>
	<br>
	  <!--a name="photo"><img HEIGHT="300" WIDTH="430" src="DSCN9553.jpg" alt="DSCN9553"/></a-->
	  <img src="./DSCN9553.jpg" alt="Vivek Kumar" height="290" width="500" /> </a>
	</div>
	
        <div class="contentText">
        <h1><span style="color: #9f8060; padding-top:8px; font-size:21px; font-weight:bold; letter-spacing:-1px; "><a name="about"> Biography</a></span></h1>

	<p>

	Vivek Kumar is an Assistant Professor at Indraprastha Institute of Information Technology, Delhi (<a href="https://iiitd.ac.in/" target="_blank">IIITD</a>). He earned his Ph.D. in Computer Science from <a href="http://cecs.anu.edu.au/" target="_blank">Australian National University</a> in January 2014 under the supervision of <a href="http://users.cecs.anu.edu.au/~steveb/" target="_blank">Prof. Steve Blackburn</a>. His Ph.D. research focused on using managed runtime techniques for improving the performance and productivity of parallel programming on multicore architectures. He has a Bachelor degree in Mechanical Engineering from <a href="http://vtu.ac.in/" target="_blank">VTU</a>, India. 

</p><p>

Prior to joining Ph.D. program in March 2010, he worked for nearly 6 years in research and development positions in High Performance Computing area at technology firms based in Bangalore such as <a href="http://www-07.ibm.com/in/isl/index.html" target="_blank">IBM Systems and Technology Labs</a> and <a href="http://www.cdac.in/index.aspx?id=research" target="_blank">C-DAC</a>. After Ph.D., he worked for nearly 3 years as a Research Scientist at <a href="http://compsci.rice.edu/" target="_blank">Rice University</a> in <a href="http://vsarkar.rice.edu/" target="_blank">Prof. Vivek Sarkar's</a> <a href="https://wiki.rice.edu/confluence/display/HABANERO/Habanero+Extreme+Scale+Software+Research+Project" target="_blank">Habanero Extreme Scale Software Research Group</a>. 

</p><p>

<b>Research Interests:</b> Parallel programming models and Runtime systems. I am leading <a href="http://hipec.iiitd.edu.in/index.html">HiPeC Lab at IIITD</a>.

	</p>

  	</p>
        </div>
	<br>
	<br>
	<br>

        <div class="contentText">
        <h1><span style="color: #9f8060; padding-top:8px; font-size:21px; font-weight:bold; letter-spacing:-1px; "><a name="news"> News </a></span></h1>
	<div style="width:670px;height:150px;line-height:3em;overflow:auto;padding:5px;border:4px  double #DEBB07;">
            <ul>
                <li>[2020]: 
		<ul>
		<li> Paper in HiPC'20 </li>
		<li> Excellence in Teaching award for CSE502</li>
		</ul>
		</li>

                <li>[2019]: 
		<ul>
		<li> Paper in EuroPar'19 </li>
		<li> Paper in IWOMP'19 </li>
		<li> Invited talk at IndoSys'19</li>
		<li> Excellence in Teaching award for CSE502 and CSE201 </li>
		</ul>
		</li>

                <li>[2018]: 
		<ul>
		<li> Excellence in Teaching award for CSE502 and CSE201 </li>
		<li> Abhiprayah Tiwari successfully defended his M.Tech. thesis </li>
		</ul>
		</li>

                <li>[2017]: 
		<ul>
                <li> Excellence in Teaching award at IIITD for CSE502 in Spring 2017</li>
                <li> Teaching Advanced Programming (CSE201) at IIITD in Fall 2017 </li>
                <li> Paper in <a href="http://www.mcs.anl.gov/events/workshops/ashes/2017/index.php">AsHES 2017</a>  </li>
		<li> Thanks to Texas Instruments for donating EVMK2H development board. </li>
		<li> Teaching a new course on parallel programming at IIITD in Spring 2017.
		</ul>
		</li>

                <li>[2016]: 
		<ul>
		 <li> In December 2016, joined <a href="https://iiitd.ac.in/">IIITD</a> as an Assistant Professor.</li>
		 <li> Paper in <a href="http://www.csm.ornl.gov/workshops/openshmem2016/" target="_blank">OpenSHMEM 2016</a>, <a href="http://pppj16.inf.usi.ch/pppj" target="_blank">PPPJ 2016</a>, <a href="http://hpc.pnl.gov/IA3/IA3/Call_For_Papers.html"  target="_blank">IA^3</a> </li>
		</ul>
		</li>

                <li>[2015]: Paper in <a href="http://ieee-hpec.org/cfp.htm" target="_blank">HPEC 2015</a>, <a href="http://hpcl.seas.gwu.edu/pgas15/" target="_blank">PGAS 2015</a>
		</li>

                <li>[2014]: 
		<ul>
		<li> In March 2014, joined Rice university as a Research Scientist </li>
		<li> Submitted my PhD thesis </li>
		<li> Paper in <a href="http://vee2014.cs.technion.ac.il/" target="_blank">VEE 2014</a>, <a href="http://nic.uoregon.edu/pgas14/index.php" target="_blank">PGAS 2014</a> </li>
		</ul>
		</li>

                <li>[2013]: 
		<ul>
		<li>OOPSLA 2012 <a href="#publications">paper</a> selected for <a href="http://www.sigplan.org/Newsletters/CACM/Papers" target="_blank">SIGPLAN Communications of ACM Research Highlights</a> </li> 
		<li> PhD research accepted for Dissertation Research Showcase track at <a href="http://sc13.supercomputing.org/content/overview" target="_blank">SC 2013</a>. </li>
		</li>
		</ul>
	
		<li>[2012]: Paper in OOPSLA 2012 </li>
		<li>[2010]: Joined PhD studies at ANU </li>

            </ul>
	</div>
	</div>
	<br>
	<br>
	<br>

<!--
        <div class="contentText"> 
	<h1><span style="color: #9f8060; padding-top:8px; font-size:21px; font-weight:bold; letter-spacing:-1px; "><a name="research"> Research </a></span></h1>
	<p> My research interests are managed runtimes, parallel programming modelscomputing, programming language design and implementation. </p> 
	</div>
	<br>
	<br>
	<br>
-->
        <div class="contentText"> 
	<h2><span style="color: #9f8060; padding-top:8px; font-size:21px; font-weight:bold; letter-spacing:-1px; "><a name="publications"> Publications </a></span></h2>
	<br>
	<h1><span style="color: #78582E; padding-top:8px; font-size:18px; font-weight:bold; letter-spacing:-1px; "> Refereed Conferences / Workshops </span></h1>

	<ul>
	<li><p>[<b>HiPC 2020</b>]: V. Kumar, "<i><b>PufferFish: NUMA-Aware Work-stealing Library using Elastic Tasks</b></i>", in 27th IEEE International Conference on High Performance Computing, Data, and Analytics, Pune, India, December 2020 (to appear). [<a href="javascript: void(0);" onClick="toggle('p15')">abstract</a>] 
	<div id="p15" style="display:none;">
	<i><p>
	Due to the challenges in providing adequate memory access to many cores on a single processor, Multi-Die and Multi-Socket based multicore systems are becoming mainstream. These systems offer cache-coherent Non-Uniform Memory Access (NUMA) across several memory banks and cache hierarchy to increase memory capacity and bandwidth. Random work-stealing is a widely used technique for dynamic load balancing of tasks on multicore processors. However, it scales poorly on such NUMA systems for memory-bound applications due to cache misses and remote memory access latency. Hierarchical Place Tree (HPT) is a popular approach for improving the locality of a task-based parallel programming model, albeit it requires the programmer to map the dynamically unfolding tasks over a NUMA system evenly. Specifying data-affinity hints provides a more natural way to map the tasks than HPT. Still, a scalable work-stealing implementation for the same is mostly unexplored for modern NUMA systems.
	</p><p>
	This paper presents PufferFish, a new asyncâ€“finish parallel programming model and work-stealing runtime for NUMA systems that provide a close coupling of the data- affinity hints provided for an asynchronous task with the HPTs in Habanero C/C++ library (HClib). PufferFish introduces Hierarchical Elastic Tasks (HET) that improves the locality by shrinking itself to run on a single worker inside a place or puffing up across multiple workers depending on the work imbalance at a particular place in an HPT. We use a set of widely used memory-bound benchmarks exhibiting regular and irregular execution graphs for evaluating PufferFish. On these benchmarks, we show that PufferFish achieves a geometric mean speedup of 1.5Ã— and 1.9Ã— over HPT implementation in HClib and random work-stealing in CilkPlus, respectively, on a 32-core NUMA AMD EPYC processor.
	</i></p></li>
	</ul>
	
	<ul>
	<li><p>[<b>IWOMP 2019</b>]: V. Kumar, A. Tiwari, and G. Mitra, "<i><b>HetroOMP: OpenMP for Hybrid Load Balancing Across Heterogeneous Processors</b></i>", in 15th International Workshop on OpenMP, Lecture Notes in Computer Science (LNCS), Springer, Auckland, New Zealand, September 2019. [<a href="javascript: void(0);" onClick="toggle('p14')">abstract</a>, <a href="./papers/IWOMP2019.pdf">paper</a>, <a href="./papers/IWOMP2019_slides.pdf">slides</a>] 
	<div id="p14" style="display:none;">
	<i><p>
	The OpenMP accelerator model enables an efficient method of offloading computation from host CPU cores to accelerator devices. However, it leaves it up to the programmer to try and utilize CPU cores while offloading computation to an accelerator. In this paper, we propose HetroOMP, an extension of the OpenMP accelerator model that supports a new clause hetro which enables computation to execute simultaneously across both host and accelerator devices using standard tasking and work-sharing pragmas.
	</p><p>
	To illustrate our proposal for a hybrid execution model, we implemented a proof-of-concept work-stealing HetroOMP runtime for the heterogeneous TI Keystone-II MPSoC. This MPSoC has host ARM CPU cores alongside accelerator digital signal processor (DSP) cores. We present the design and implementation of the HetroOMP runtime and use several well-known benchmarks to demonstrate that HetroOMP achieves a geometric mean speedup of 3.6Ã— compared to merely using the OpenMP accelerator model.
	</i></p></li>
	</ul>
	
	<ul>
	<li><p>[<b>EuroPar 2019</b>]: V. Kumar, "<i><b>Featherlight Speculative Task Parallelism</b></i>", in 25th International European Conference on Parallel and Distributed Computing, Lecture Notes in Computer Science (LNCS), Springer, GÃ¶ttingen, Germany, August 2019. [<a href="javascript: void(0);" onClick="toggle('p13')">abstract</a>, <a href="./papers/europar2019.pdf">paper</a>, <a href="./papers/europar2019_slides.pdf">slides</a>, <a href="./talk/europar2019.m4a" target="_blank"><img src="./sound.png" width="25" height="25" alt="Talk" /></a>] 
	<div id="p13" style="display:none;">
	<i><p>
	Speculative task parallelism is a widely used technique for solving search based irregular computations such as graph algorithms. Here, tasks are created speculatively to traverse different search spaces in parallel. Only a few of these tasks succeed in finding the solution, after which the remaining tasks are canceled. To ensure timely cancellation of tasks, existing frameworks either require programmer introduced cancellation checks inside every method in the call chain, thereby hurting the productivity, or provide limited parallel performance.
	</p><p>
	In this paper we propose Featherlight, a new programming model for speculative task parallelism that satisfies the serial elision property and doesn't require any task cancellation checks. We show that \mysys improves the productivity through a classroom based study. Further, to support Featherlight we present the design and implementation of a task cancellation technique that exploits runtime mechanisms already available within managed runtimes and achieves a geometric mean speedup of 1.6x over the popular Java Fork/Join framework on a 20 core machine.
	</i></p></li>
	</ul>
	
	<ul>
	<li><p>[<b>IPDPSW 2017</b>]: M. Grossman, V. Kumar, N. Vrvilo, Z. Budimlic, and V. Sarkar, "<i><b>A Pluggable Framework for Composable HPC Scheduling Libraries</b></i>", in Proceedings of IEEE International Parallel and Distributed Processing Symposium Workshops, ACM, Orlando, Florida, USA, May 2017. [<a href="javascript: void(0);" onClick="toggle('p12')">abstract</a>, <a href="./papers/ashes2017.pdf">paper</a>, <a href="./papers/ashes17-slides.pdf">slides</a>] 
	<div id="p12" style="display:none;"><i><p>
	Driven by the increasing diversity of current and future HPC hardware and software platforms, the HPC community has seen a dramatic increase in research and development efforts into the composability of discrete software systems. While modularity is often desirable from a software engineering, quality assurance, and maintainability perspective, the barriers between software components often hide optimization opportunities. Recent examples of work in composable HPC software include GPU-Aware MPI, OpenMPâ€™s target directive, Lithe, HCMPI, and MVAPICHâ€™s unified communication runtime. These projects all deal with breaking down the walls between software or hardware components in order to achieve performance, programmability, and/or portability gains. However, they also generally focus on composing only specific types of HPC software and have limited extensability.
	</p><p>
	In this paper, we present work on using a pluggable API framework on top of a "generalized work-stealing" runtime to achieve composability of communication, accelerator, and other HPC libraries. We motivate this work by the increasing heterogeneity of HPC hardware, software, and applications, and note that as heterogeneity increases many discrete software frameworks will need to cooperate within a single process. Our framework, called HiPER (a Highly Pluggable, Extensible, and Re-configurable scheduling framework for HPC) enables exactly this cooperation.
	</p><p>
	We demonstrate the programmability improvements enabled by the HiPER framework through the use of novel APIs which reduce programmer burden. We also present performance studies that demonstrate that through unified and asynchronous scheduling of composed software systems we can achieve performance improvements over hand-optimized benchmark.
	</i></p></li>
	</ul>
	
	<ul>
	<li><p>[<b>IA<sup>3</sup> 2016</b>]: V. Kumar, K. Murthy, V. Sarkar, and Y. Zheng, "<i><b>Optimized Distributed Work-Stealing</b></i>", in Proceedings of the 6th International Workshop on Irregular Applications: Architectures and Algorithms, <a href="http://hpc.pnl.gov/IA3/IA3/Call_For_Papers.html"  target="_blank">IA<sup>3</sup></a>, ACM, Salt Lake City, Utah, USA, November 2016 (co-located with SC16). [<a href="javascript: void(0);" onClick="toggle('p11')">abstract</a>, <a href="./papers/ia3-kumar-2016.pdf">paper</a>, <a href="papers/slides_ia3_2016.pdf">slides</a>, <a href="./talk/ia3_2016.m4a" target="_blank"><img src="./sound.png" width="25" height="25" alt="Talk" /></a>] 
	<div id="p11" style="display:none;"><i><p>

	Work-stealing is a popular approach for dynamic load balancing of task-parallel programs. However, as has been widely studied, the use of classical work-stealing algorithms on massively parallel and distributed supercomputers introduces several performance issues. One such issue is the overhead of failed steals (communicating with a victim that has no work), which is far more severe in the distributed context than within a single SMP node. Due to the cost of inter node communication, it is critical to reduce the number of failed steals in a distributed context. Prior work has demonstrated that load-aware victim processor selection can reduce the number of failed steals, but not eliminate the failed steals completely.
	</p> <p>

In this paper, we present two different load-aware implementations of distributed work-stealing algorithm in HabaneroUPC++ PGAS library - BaselineWS and SuccessOnlyWS. BaselineWS follows prior work in implementing a load-aware distributed work-stealing strategy. SuccessOnlyWS implements a novel load- aware distributed work-stealing strategy that overcomes failed attempts by introducing a new policy for moving work from busy to idle processors. In contrast to BaselineWS, SuccessOnlyWS also avoids querying the same processor multiple times with failed steals.We evaluate both BaselineWS and SuccessOnlyWS by using up to 12288 cores of Edison, a CRAY-XC30 supercomputer and by using dynamic irregular applications, as exemplified by the UTS and NQueens benchmarks. We demonstrate that SuccessOnlyWS provides performance improvements up to 7% over BaselineWS.	

	</p></i></li>
        </ul>
	
        <ul>
	<li><p>[<b>PPPJ 2016</b>]: V. Kumar, J. Dolby, and S. M. Blackburn, "<i><b>Integrating Asynchronous Task Parallelism and Data-centric Atomicity</b></i>", at The 13th International Conference on Principles and Practices of Programming on the Java Platform: Virtual Machines, Languages, and Tools, <a href="http://pppj16.inf.usi.ch/pppj" target="_blank">PPPJ'16</a>, Lugano, Switzerland, August 2016. [<a href="javascript: void(0);" onClick="toggle('p10')">abstract</a>, <a href="http://users.cecs.anu.edu.au/~steveb/downloads/pdf/ws-pppj-2016.pdf">paper</a>, <a href="./papers/slides_pppj16.pdf">slides</a>, <a href="./talk/pppj16.m4a" target="_blank"><img src="./sound.png" width="25" height="25" alt="Talk" /></a>]
	<div id="p10" style="display:none;"><i><p> 
	Processor design has turned toward parallelism and heterogeneous cores to achieve performance and energy efficiency. Developers find high-level languages attractive as they use abstraction to offer productivity and portability over these hardware complexities. Over the past few decades, researchers have developed increasingly advanced mechanisms to deliver performance despite the overheads naturally imposed by this abstraction. Recent work has demonstrated that such mechanisms can be exploited to attack overheads that arise in emerging high-level languages, which provide strong abstractions over parallelism. However, current implementation of existing popular high-level languages, such as Java, offer little by way of abstractions that allow the developer to achieve performance in the face of extensive hardware parallelism. 
	</p> <p>
	In this paper, we present a small set of extensions to the Java programming language that aims to achieve both high performance and high productivity with minimal programmer effort. We incorporate ideas from languages like X10 and AJ to develop five annotations in Java for achieving asynchronous task parallelism and data-centric concurrency control. These annotations allow the use of a highly efficient implementation of a work-stealing scheduler for task parallelism. We evaluate our proposal by refactoring classes from a number of existing multithreaded open source projects to use our new annotations. Our results suggest that these annotations significantly reduce the programming effort while achieving performance improvements up to 30% compared to conventional approaches. 
	</p></i></li>
        </ul>

        <ul>
	<li><p>[<b>OpenSHMEM 2016</b>]: M. Grossman, V. Kumar, Z. Budimlic, and V. Sarkar, "<i><b>Integrating Asynchronous Task Parallelism with OpenSHMEM</b></i>", at The 3rd workshop on OpenSHMEM and Related Technologies, <a href="http://www.csm.ornl.gov/workshops/openshmem2016/" target="_blank">OpenSHMEM 2016</a>, Baltimore, Maryland, USA, August 2016. [<a href="javascript: void(0);" onClick="toggle('p9')">abstract</a>, <a href="./papers/asyncshmem1.pdf">paper</a>, <a href="http://www.csm.ornl.gov/workshops/openshmem2016/Presentations/P19_Integrating_Asynchronous_Task_Parallelism_with_OpenSHMEM.pdf">slides</a>]
	<div id="p9" style="display:none;"><i><p> The Partitioned Global Address Space (PGAS) programming models combine shared and distributed memory features, providing the basis for high-performance and high-productivity parallel programming environments. Most current PGAS approaches use complex compiler transformations to translate the user code to native code. OpenSHMEM is a very widely used PGAS programming model that offers a library based approach. Currently, OpenSHMEM relies on other libraries (e.g., OpenMP) for harnessing node-level parallelism. This OpenSHMEM+X approach requires the expertise of a hero-level programmer and typically encounters bottlenecks on shared resources, long wait times due to load imbalances, as well as data locality problems. </p>
	<p> In this paper, we introduce an AsyncSHMEM PGAS library that supports a tighter integration of shared and distributed memory parallelism than approaches based on OpenSHMEM+X. AsyncSHMEM integrates the OpenSHMEM library with a thread-pool-based work-stealing runtime. AsyncSHMEM aims to prepare OpenSHMEM for the next generation of HPC systems by making it more adaptive and taking advantage of asynchronous computation to hide data transfer latencies, interoperate with tasks, improve load balancing (both of communication and computation), and improve locality. In this paper we present the design and implementation of AsyncSHMEM, and demonstrate the performance of AsyncSHMEM by performing a scalability analysis of two 
benchmarks on the Titan supercomputer. Our experiments show that AsyncSHMEM is competitive with OpenSHMEM+OpenMP model when executing highly regular workloads, while it significantly outperforms it on highly event-driven applications. </p></i></li>
	</ul>

	<ul>
	<li><p>[<b>HPEC 2015</b>]: V. Kumar, A. Sbirlea, Z. Budimlic, D. Majeti and V. Sarkar, "<i><b>Heterogeneous Work-stealing across CPU and DSP cores</b></i>", at The 19th International Conference on High Performance Extreme Computing Conference, <a href="http://ieee-hpec.org/index.htm" target="_blank">HPEC 2015</a>, Waltham, MA, USA, September 2015. [<a href="javascript: void(0);" onClick="toggle('p7')">abstract</a>, <a href="./papers/kumar_hpec2015.pdf">paper</a>, <a href="./papers/slides_hpec2015.pdf">slides</a>, <a href="./talk/hpec15.m4a" target="_blank"><img src="./sound.png" width="25" height="25" alt="Talk" /></a>] 
	<div id="p7" style="display:none;"><i> <p>Due to the increasing power constraints and higher and higher performance demands, many vendors have shifted their focus from designing high-performance computer nodes using powerful multicore general-purpose CPUs, to nodes containing a smaller number of general-purpose CPUs aided by a larger number of more power-efficient special purpose processing units, such as GPUs, FPGAs or DSPs. While offering a lower power-to-performance ratio, unfortunately, such heterogeneous systems are notoriously hard to program, forcing the users to resort to lower-level direct programming of the special purpose processors and manually managing data transfer and synchronization between the parts of the program running on general-purpose CPUs and on special-purpose processors.</p>

	<p>In this paper, we present HC-K2H, a programming model and runtime system for the Texas Instruments Keystone II Hawking platform, consisting of 4 ARM CPUs and 8 TI DSP processors. This System-on-a-Chip (SoC) offers high floating-point performance with lower power requirements than other processors with comparable performance. We present the design and implementation of a hybrid programming model and work-stealing runtime that allows tasks to be created and executed on both the ARM and DSP, and enables the seamless execution and synchronization of tasks regardless of whether they are running on the ARM or DSP. The design of our programming model and runtime is based on an extension of the Habanero C programming system. We evaluate our implementation using task-parallel benchmarks on a Hawking board, and demonstrate excellent scaling compared to sequential implementations on a single ARM processor.</p></i></li>
	</ul>

	<ul>
	<li><p>[<b>PGAS 2014</b>]: V. Kumar, Y. Zheng, V. Cave, Z. Budimlic and V. Sarkar, "<i><b>HabaneroUPC++: a Compiler-free PGAS Library</b></i>", at The 8th International Conference on Partitioned Global Address Space Programming Models, <a href="http://nic.uoregon.edu/pgas14/index.php" target="_blank">PGAS 2014</a>, Eugene, Oregon, October 2014. [<a href="javascript: void(0);" onClick="toggle('p6')">abstract</a>, <a href="./papers/pgas14.pdf">paper</a>, <a href="./papers/slides_pgas14.pdf">slides</a>, <a href="./talk/pgas14_talk.m4a" target="_blank"><img src="./sound.png" width="25" height="25" alt="Talk" /></a>] 
	<div id="p6" style="display:none;"><i> <p>The Partitioned Global Address Space (PGAS) programming models combine shared and distributed memory features, providing the basis for high performance and high productivity parallel programming environments. UPC++ is a very recent PGAS implementation that takes a library-based approach and avoids the complexities associated with compiler transformations. However, this implementation does not support dynamic task parallelism and only relies on other threading models (e.g., OpenMP or pthreads) for exploiting parallelism within a PGAS place. </p>

	<p>In this paper, we introduce a compiler-free PGAS library called Habanero-UPC++, which supports a tighter integration of intra-place and inter-place parallelism than standard hybrid programming approaches. The library makes heavy use of C++11 lambda functions in its APIs. C++11 lambdas avoid the need for compiler support while still retaining the syntactic convenience of language-based approaches. The  Habanero-UPC+ library implementation is based on a tight integration of the UPC++ library and the Habanero-C++ library, with new extensions to support the integration. The UPC++ library is used to provide PGAS communication and function shipping support using GASNet, and the Habanero-C++ library is used to provide support for intra-place work-stealing integrated with function shipping. We demonstrate the programmability and performance of our implementation using two benchmarks, scaled up to 6000 cores. The insights developed in this paper promise to further enhance the usability and popularity of PGAS programming models.</p></i></li>
        </ul>

	<ul>
	<li><p>[<b>VEE 2014</b>]: V. Kumar, S. M. Blackburn and D. Grove, "<i><b>Friendly Barriers: Efficient Work-Stealing With Return Barriers</b></i>", at The 10th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments, <a href="http://vee2014.cs.technion.ac.il/" target="_blank">VEE 2014</a>, Salt Lake City, Utah, March 2014. [<a href="javascript: void(0);" onClick="toggle('p5')">abstract</a>, <a href="./papers/vee2014.pdf">paper</a>, <a href="./papers/slides_vee14.pdf">slides</a>]
	<div id="p5" style="display:none;"><i> <p> This paper addresses the problem of efficiently supporting parallelism within a managed runtime. A popular approach for exploiting software parallelism on parallel hardware is task parallelism, where the programmer explicitly identifies potential parallelism and the runtime then schedules the work. Work-stealing is a promising scheduling strategy that a runtime may use to keep otherwise idle hardware busy while relieving overloaded hardware of its burden. However, work stealing comes with substantial overheads. Recent work identified sequential overheads of work-stealing, those that occur even when no stealing takes place, as a significant source of overhead. That work was able to reduce sequential overheads to just 15%.</p>

<p> In this work, we turn to dynamic overheads, those that occur each time a steal takes place. We show that the dynamic overhead is dominated by introspection of the victimâ€™s stack when a steal takes place. We exploit the idea of a low overhead return barrier to reduce the dynamic overhead by approximately half, resulting in total performance improvements of as much as 20%. Because, unlike prior work, we attack the overheads directly due to stealing and therefore attack the overheads that grow as parallelism grows, we improve the scalability of work-stealing applications. This result is complementary to recent work addressing the sequential overheads of work-stealing. This work therefore substantially relieves work-stealing of the increasing pressure due to increasing intranode hardware parallelism. 
	</p></i></li>
	</ul>
	<ul>
	<li><p>[<b>OOPSLA 2012</b>]: V. Kumar, D. Frampton, S. M. Blackburn, D. Grove, and O. Tardieu, "<i><b>Work-Stealing Without The Baggage</b></i>," in Proceedings of the 2012 ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages & Applications (<a href="http://splashcon.org/2012/program/oopsla-research-papers">OOPSLA 2012</a>), Tucson, AZ, October 19-26, 2012, 2012. (</a> <b>selected for <a href="http://www.sigplan.org/Newsletters/CACM/Papers" target="_blank">SIGPLAN Communications of ACM Research Highlights</a>, 2013</b>) [<a href="javascript: void(0);" onClick="toggle('p2')">abstract</a>, <a href="./papers/ws-oopsla-2012-1.pdf">paper</a>, <a href="./papers/oopsla2012.pdf">slides</a>, <a href="./talk/oopsla2012talk.m4a" target="_blank"><img src="./sound.png" width="25" height="25" alt="Talk" /></a>, <a href="http://cs.anu.edu.au/~vivek/ws-oopsla-2012/" target="_blank"> benchmarks </a>] 
	<div id="p2" style="display:none;"><i> <p> Work-stealing is a promising approach for effectively exploiting software parallelism on parallel hardware. A programmer who uses work-stealing explicitly identifies potential parallelism and the runtime then schedules work, keep- ing otherwise idle hardware busy while relieving overloaded hardware of its burden. Prior work has demonstrated that work-stealing is very effective in practice. However, work- stealing comes with a substantial overhead: as much as 2Ã— to 12Ã— slowdown over orthodox sequential code.</p>
<p>In this paper we identify the key sources of overhead in work-stealing schedulers and present two significant refinements to their implementation. We evaluate our work- stealing designs using a range of benchmarks, four different work-stealing implementations, including the popular fork-join framework, and a range of architectures. On these benchmarks, compared to orthodox sequential Java, our fastest design has an overhead of just 15%. By contrast, fork-join has a 2.3Ã— overhead and the previous implementation of the system we use has an overhead of 4.1Ã—. These results and our insight into the sources of overhead for work- stealing implementations give further hope to an already promising technique for exploiting increasingly available hardware parallelism.</p>
  </i></div></p></li>
	</ul>

	<ul>
	<li><p>[<b>VMIL 2012</b>]: V. Kumar and S. M. Blackburn, "<i><b>Faster Work-Stealing With Return Barriers</b></i>", at The 6th workshop on Virtual Machines and Intermediate Languages, <a href="http://design.cs.iastate.edu/vmil/2012/" target="_blank">VMIL 2012</a>, Tucson, AZ, October 2012. [<a href="javascript: void(0);" onClick="toggle('p3')">abstract</a>, <a href="./papers/vmil2012_paper.pdf">paper</a>, <a href="./papers/vmil2012.pdf">slides</a>] 
	<div id="p3" style="display:none;"><i> <p> Work-stealing is a promising approach for effectively exploiting software parallelism on parallel hardware. A programmer who uses work-stealing explicitly identifies potential parallelism and the runtime then schedules work, keep- ing otherwise idle hardware busy while relieving overloaded hardware of its burden. However, work-stealing comes with substantial overheads. Our prior work has demonstrated that using the exception handling mechanism of modern VMs and gathering the runtime information directly from the victims execution stack can significantly reduce these overheads. A return barrier is a mechanism for intercepting the popping of a stack frame, and thus is a powerful tool for optimizing mechanisms that involve scanning of stack state.</p>
	<p> In this paper we identify the overhead associated with managing the work-stealing related information on a victimâ€™s execution stack. We present the design and preliminary findings of using return barriers on a victimâ€™s execution stack to reduce these overheads. We evaluate our design using classical work-stealing benchmarks. On these bench- marks, compared to our prior design, we are able to reduce the overheads by as much as 58%. These preliminary findings give further hope to an already promising technique of harnessing rich features of a modern VM inside a work-stealing scheduler. </p></i></div>
	</p></li>
	</ul>

	<ul>
	<li><p>[<b>X10 2011</b>]: V. Kumar, D. Frampton, D. Grove, O. Tardieu, and S. M. Blackburn, "<i><b>Work-Stealing by Stealing States from Live Stack Frames of a Running Application</b></i>," at <a href="http://x10-lang.org/workshop/cfp/program.html" target="_blank">X10' 11 Workshop</a> collocated with PLDI 2011, San Jose, CA, June 2011. [<a href="javascript: void(0);" onClick="toggle('p1')">abstract</a>, <a href="./papers/x10-2011ws.pdf" target="_blank">paper</a>, <a href="http://x10.sourceforge.net/documentation/papers/X10Workshop2011/Kumarx10ws.pdf" target="_blank">slides</a>]
<div id="p1" style="display:none;"><i> <p>The use of a work stealing scheduler has become a popular approach for providing task parallelism. It is used in many modern parallel programming languages, such as Cilk and X10, which have emerged to address the concerns of parallel programming complexity on modern multicore architectures. </p>
	<p>There are various challenges in providing an efficient implementation of work-stealing, but in any implementation it must be possible for the thief to access the execution state required to per- form the stolen task. The natural way to achieve this is to save the necessary state whenever a producer creates stealable work. While the ability to provide some degree of parallelism may dominate performance at scale, it is common for the vast majority of potentially stealable work to never actually be stolen, but instead processed by the producer itself. This indicates that to further improve performance we should minimize the overheads incurred in making work available for stealing. </p>
	<p>We are not the only ones to make this observation, for example X10's current C++ work-stealing implementation stack-allocates state objects and lazily copies them to the heap to avoid unnecessary heap allocation during normal execution. In our context of a Java virtual machine, it is possible to extend this idea further and avoid stack allocating state objects, but instead allow thieves to ex- tract state directly from within stack frames of the producer. This is achieved by using state-map information provided by a cooperative runtime compiler, allowing us to drive down the cost of making state available for stealable work items. We discuss our design and preliminary findings for the implementation of our framework in- side X10 work-stealing runtime and the baseline compiler of Jikes RVM, a high-performance Java research virtual machine. </p> </i></div></p></li>
	</ul>

	<h1><span style="color: #78582E; padding-top:8px; font-size:18px; font-weight:bold; letter-spacing:-1px; "> Posters </span></h1>

	<ul>
	<li><p> V. Kumar, M. Grossman, H. Shan and V. Sarkar, "<i><b>Scaling HabaneroUPC++ on Heterogeneous Supercomputers</b></i>", at The 9th International Conference on Partitioned Global Address Space Programming Models, <a href="http://hpcl.seas.gwu.edu/pgas15/" target="_blank">PGAS 2015</a>, (poster accepted with extended abstract), Washington, D.C., September 2015. [<a href="javascript: void(0);" onClick="toggle('p8')">abstract</a>, <a href="./papers/pgas15.pdf">paper</a>, <a href="./papers/pgas15_poster.pdf">poster</a>]
	<div id="p8" style="display:none;"><i> <p>Accelerators/co-processors have made their way into supercomputing systems. These modern heterogeneous systems features multiple layers of memory hierarchies, and produce a high degree of thread-level parallelism. To ensure that current and future applications perform well on these systems, it is important that users be able to cleanly express the various types of parallelism found in their applications while trusting that expertly-implemented runtime libraries will schedule this parallelism in a way that efficiently utilizes the memory hierarchies and computational resources of their system.</p>

	<p> In this poster we present our work-in-progress, a distributed, heterogeneous programming model in HabaneroUPC++, which aims to target distributed, heterogeneous systems with multi-layered memory hierarchies through a distributed data-driven programming model that integrates all memory layers together through data-flow programming.
	</p></i></li>
	</ul>

		<ul>
	<li><p> V. Kumar, "<i><b> Rule The Next Generation Supercomputers With X10</b></i>", at ANU CECS HDR poster presentation (<b><i> first prize recipient</b></i>), 2010. [<a href="./papers/hdr2010.pdf">poster</a>]
		</ul>
	
	<h1><span style="color: #78582E; padding-top:8px; font-size:18px; font-weight:bold; letter-spacing:-1px; "> Ph.D. Thesis</span></h1>
	<p><i><b>Achieving High Performance and High Productivity in Next Generation Parallel Programming Languages</i></b>, Ph.D. thesis, The Australian National University, Canberra, Australia, degree awarded in May 2015. [<a href="http://hdl.handle.net/1885/145103">link</a>]</p>
	
	</div>
	<br>
	<br>
	<br>

        <div class="contentText"> 
	<h1><span style="color: #9f8060; padding-top:8px; font-size:21px; font-weight:bold; letter-spacing:-1px; "><a name="talks"> Talks </a></span></h1>

	<ul>
	<li><p> <i>Featherlight Speculative Task Parallelism</i> <a href="https://2019.euro-par.org/"  target="_blank">EuroPar</a>, Gottingen, Germany, 2019. [<a href="./papers/europar2019_slides.pdf">slides</a>, <a href="./talk/europar2019.m4a" target="_blank"><img src="./sound.png" width="25" height="25" alt="Talk" /></a>] </p></li> 
	<li><p> <i>Structured Parallelism for High Productivity and High Performance</i> <a href="http://cds.iisc.ac.in/faculty/simmhan/conf/indosys-2019/index.html"  target="_blank">IndoSys</a>, IISc, Bangalore, 2019. [<a href="http://cds.iisc.ac.in/faculty/simmhan/conf/indosys-2019/vk.pdf">slides</a>] </p></li> 
	<li><p> <i>Integrating Asynchronous Task Parallelism and Data-centric Atomicity</i> <a href="http://pppj16.inf.usi.ch/pppj"  target="_blank">PPPJ</a>, Lugano, Switzerland, 2016. [<a href="./papers/slides_pppj16.pdf">slides</a>, <a href="./talk/pppj16.m4a" target="_blank"><img src="./sound.png" width="25" height="25" alt="Talk" /></a>] </p></li> 
	<li> <p> <i>Heterogeneous Work-stealing across CPU and DSP cores</i>, <a href="http://ieee-hpec.org/index.htm" target="_blank">HPEC</a>, Waltham, MA, USA, 2015. [<a href="./papers/slides_hpec2015.pdf">slides</a>, <a href="./talk/hpec15.m4a" target="_blank"><img src="./sound.png" width="25" height="25" alt="Talk" /></a>]  </p> </li>
	<li> <p> <i>HabaneroUPC++: a Compiler-free PGAS Library</i>, <a href="http://nic.uoregon.edu/pgas14/index.php" target="_blank">PGAS</a>, Eugene, Oregon, USA, 2014. [<a href="./papers/slides_pgas14.pdf">slides</a>, <a href="./talk/pgas14_talk.m4a" target="_blank"><img src="./sound.png" width="25" height="25" alt="Talk" /></a>] </p> </li>
	<li><p><i>High Performance Runtime for Next Generation Parallel Programming Languages</i>", <a href="http://sc13.supercomputing.org/schedule/event_detail.php?evid=drs111" target="_blank">Doctoral Showcase - Dissertation Research Showcase</a>, <a href="http://sc13.supercomputing.org/"  target="_blank">SC</a>, Denver, Colorado, USA, 2013. [<a href="./papers/slides_sc13.pdf">slides</a>, <a href="./talk/audio_sc13.m4a" target="_blank"><img src="./sound.png" width="25" height="25" alt="Talk" /></a>]
	<li><p> <i>Work-Stealing Without The Baggage</i>, <a href="http://splashcon.org/2012/program/oopsla-research-papers">OOPSLA</a>, Tucson, AZ, USA, 2012. [<a href="./papers/oopsla2012.pdf">slides</a>, <a href="./talk/oopsla2012talk.m4a" target="_blank"><img src="./sound.png" width="25" height="25" alt="Talk" /></a>] </p></li>
	<li><p> <i>Faster Work-Stealing With Return Barriers</i>, <a href="http://design.cs.iastate.edu/vmil/2012/" target="_blank">VMIL</a>, Tucson, AZ, USA, 2012. [<a href="./papers/vmil2012.pdf">slides</a>] </p></li>
	<li><p><i>Work-Stealing by Stealing States from Live Stack Frames of a Running Application</i>, <a href="http://x10-lang.org/workshop/cfp/program.html" target="_blank">X10'11</a>, San Jose, CA, USA, 2011. [<a href="http://x10.sourceforge.net/documentation/papers/X10Workshop2011/Kumarx10ws.pdf" target="_blank">slides</a>] </p></li>
	<li><p> <i>Introduction to High Performance Computing and Message Passing Interface (MPI) </i>
		<ul>
		<li>India Meteorological Department, New Delhi, India, 2010. </li>
		<li>Birla Institute of Technology & Science, Pilani, Rajasthan, India, 2009. </li>
		<li>National Institute of Technology, Jamshedpur, India, 2009. </li>
		<li>Saha Institute of Nuclear Physics, Kolkata, India, 2008. </li>
		<li>National Metallurgical Laboratories, Jamshedpur, India, 2005. </li>
		</ul>
	</p></li>
	</ul>
	</div>
	<br>
	<br>
	<br>

	<!--
        <div class="contentText"> 
	<h1><span style="color: #9f8060; padding-top:8px; font-size:21px; font-weight:bold; letter-spacing:-1px; "><a name="tutoring"> Tutoring@ANU </a></span></h1>
	<p> Introductory Programming In Java (COMP6700) </p>
	<p> Introduction to Programming and Algorithms (COMP1100) </p>
	<p> Programming for Scientists (COMP1730) </p> 
	</div>
	<br>
	<br>
	<br>
	-->

	<div class="contentText">
        <h1><span style="color: #9f8060; padding-top:8px; font-size:21px; font-weight:bold; letter-spacing:-1px; "><a name="teaching"> Teaching </a></span></h1>
	<p> <b> IIITD </b> </p>
	<p> 
	<ul>
	<li> Parallel Runtimes for Modern Processors: Fall 2020</li>
	<li> Advanced Programming (CSE201): Fall 2017-2020</li>
	<li> Foundations of Parallel Programming (CSE502): Spring 2017-2020 </li>
	</ul>	
	</p>

<!--
	<p><b> Tutoring@ANU </b></p>
	<p>
	<ul>
        <li>Introductory Programming In Java (COMP6700)</li>
	<li>Introduction to Programming and Algorithms (COMP1100) </li>
	<li>Programming for Scientists (COMP1730)</li>
	</ul>
	</p>
-->	
	</div>
        <br>
        <br>

	<div class="contentText">
        <h1><span style="color: #9f8060; padding-top:8px; font-size:21px; font-weight:bold; letter-spacing:-1px; "><a name="students"> Students </a></span></h1>

<!--	<font color="red"> <p> <b> Current Openings </b> </p>
	<p> I am looking for PhD students! If you are very good in programming and also like hacking big software systems then apply for the upcoming <a href="https://iiitd.ac.in/admission/phd/dec2017"> PhD admission round at IIIT-Delhi whose deadline is November 22, 2017</a>. The topic you need to choose is "Parallel Programming for Multicore Processors and Accelerators". 
        </p> </font>
-->

	<p> <b> Current Students (IIITD) </b> </p>
	<ul>
	<li> Sunil Kumar (B.Tech.) : Applying machine learning techniques in HPC </li>
	<li> Akshat Gupta (B.Tech.) : Applying machine learning techniques in HPC </li>
	<li> Hardik Saini (B.Tech.): Parallelizing a Community Detection Algorithm </li>
	<li> Dibya Gautam (B.Tech.): Developing parallel programs using HClib </li>
	<li> Vibhu Agarwal (B.Tech.): Developing parallel programs using HClib </li>
	<li> Anunay Yadav (B.Tech.): GC Compaction on GPU </li>
	<li> Shubham Mittal (B.Tech.): GC Compaction on GPU </li>
	</ul>	
	</p>

	<p> <b> Past Students (IIITD) </b> </p>
	<ul>
	<li> [2020] Savit Gupta (B.Tech.): GC Compaction Library </li>
	<li> [2019] Shubham Thakral (B.Tech.): Exploring HPC in Deep Learning Frameworks </li>
	<li> [2019] Harsh Pathak and Sujay Raj (M.Tech.): Enhancing GC Compaction </li>
	<li> [2019] Gaurav Joshi and Rajat Mahey (M.Tech.): Benchmarks and Runtime for SoC </li>
	<li> [2019] Parimi Viraj (B.Tech.): Exploring HPC in Deep Learning Frameworks </li>
	<li> [2019] Abhijeet Singh (B.Tech.): Performance study of MiniFE Finite Element Mini-Application by using HClib </li>
	<li> [2018] Sai Kishore Pachigolla (B.Tech.): JVM support for data race detection in async-finish program </li>
	<li> [2018] Abhiprayah Tiwari (M.Tech.): High Performance and Energy Optimal Parallel Programming on CPU and DSP based MPSoC</li>
	<li> [2017] Vaibhav Pande (M.Tech.): Survey of parallel programming approaches in JavaScript and design of Google's V8 JavaScript engine </li>
	<li> [2017] Alind Khare (B.Tech.): Speculative parallelism based PageRank algorithm </li>
	<li> [2017] Avadh Yadav and Burhan Nabi (B.Tech.): Execution of VTK-m using HClib runtime</li>
	</ul>	
	</p>

	<p> <font color="red"> <b> Prospective Students: </b> I am looking for highly motivated B.Tech, M.Tech, and Ph.D. students to work in the following areas of parallel programming models and runtime systems: a) heterogeneous processors and accelerators (SoCs, FPGAs, etc.), and b) managed language runtimes such as JVM and JavaScript engine. If you are very good in programming (C, C++, Java) and also like hacking big software systems then please email me your resume. </p>
	<p><h3>PhD positions available: <a href="http://hipec.iiitd.edu.in/openings.html">http://hipec.iiitd.edu.in/openings.html</a></h3> </p>
        <p> 
<!--
	<p> <font color="red"> <b> Prospective Students: </b> I am looking for highly motivated B.Tech, M.Tech, and Ph.D. students to work in the following areas of parallel programming models and runtime systems: a) heterogeneous processors and accelerators (SoCs, FPGAs, etc.), and b) managed language runtimes such as JVM and JavaScript engine. If you are very good in programming (C, C++, Java) and also like hacking big software systems then please do the following. </p>
        <p> 
        <ul>
        <li> First look at my research, publications and my student projects to get a sense of my research directions </li>
        <li> Mail me your resume </li>
        </ul>
-->
   	</font>
        <!--My projects tend to be programming intensive (C, C++, Java). -->
<!--        All of my projects are under collaboration with industrial and academic partners. My current partners include IBM TJ Watson Research, Shell R&D and NCI Australia.
-->
        </p> 
<!--
	<p><font color="red">I have an opening for summer (May-July 2018) research project. If you are interested then email me your resume with subject line as "summer research project". This will be a programming intensive project in C/C++ language. </font> </p>
-->
<!--	<p><b><font color="green">Openings (August 2018): I have an immediate opening for a Research Assistant to work on a 3 months programming intensive research project. Candidate should be a) having BTech/MTech in CSE, b) very good in programming (C++/Java), and c) comfortable to stay in IIITD campus hostel for the entire project duration. If you are interested then email me your resume with subject line as "Application for research assistant". </font> </b></p> -->
	</div>
        <br>
        <br>

        <div class="contentText"> 
	<h1><span style="color: #9f8060; padding-top:8px; font-size:21px; font-weight:bold; letter-spacing:-1px; "><a name="awards"> Awards / Honors </a></span></h1>	
	<ul>
        <li> Excellence in Teaching award at IIITD (2017-2019)</li>
	<li> Ph.D. dissertation selected for <a href="http://sc13.supercomputing.org/schedule/event_detail.php-evid=drs111.html" target="_blank">Dissertation Showcase</a> track in <a href="http://sc13.supercomputing.org/index.html" target="_blank">SC13</a>,  November 2013. 
	<li> Australian National University Miscellaneous Scholarship, October 2013. 
	<li> <a href="http://dl.acm.org/citation.cfm?id=2384639" target="_blank">OOPSLA 2012 paper</a> selected for <a href="http://www.sigplan.org/Newsletters/CACM/Papers/" target="_blank">SIGPLAN Research Highlights</a>, May 2013.  
<!--        <li><p> Student travel grant for SC13 Dissertation Showcase track, November 2013.  
	<li> Australian National University Miscellaneous Scholarship, May 2013. 
	<li> <a href="http://www.sigplan.org/pac.htm" target="_blank">ACM SIGPLAN PAC</a> student travel grant, July 2012. 
	<li> Vice-Chancellor's Higher Degree Research Travel Grant, July 2012. 
-->
	<li> First prize in ANU CECS HDR student <a href="http://cecs.anu.edu.au/current_students/graduates/posters/2010" target="_blank">poster presentation 2010</a>. 
	<li> Australian National University Higher Degree Research merit scholarship (2010 - 2014). 
	<li> Australian Postgraduate Award (Industry) from <a href="http://www.arc.gov.au/" target="_blank">Australian Research Council</a> (2010 - 2013)
	</ul>
	</p>
	</div>
	<br>
	<br>

	<!--
        <div class="contentText"> 
	<h1><span style="color: #9f8060; padding-top:8px; font-size:21px; font-weight:bold; letter-spacing:-1px; "><a name="links"> Links </a></span></h1>
	<p>  
	<ul>
		<li><p>	<a href="http://www.jikesrvm.org/" target="_blank"> Jikes RVM </a></p></li>
		<li><p>	<a href="http://x10-lang.org/home/introduction1.html" target="_blank"> X10 Language </a></p></li>
		<li><p>	<a href="http://cs.anu.edu.au/~Josh.Milthorpe/x10.html" target="_blank"> X10 ANUChem benchmarks </a></p></li>
		<li><p>	<a href="http://gee.cs.oswego.edu/dl/papers/fj.pdf" target="_blank"> Java Fork/Join Framework </a></p></li>
	</ul>
	</p>
	</div>
	<br>
	<br>
	<br>
	-->

        <div class="contentText"> 
	<h1><span style="color: #9f8060; padding-top:8px; font-size:21px; font-weight:bold; letter-spacing:-1px; "><a name="software"> Open Source Software Contributions</a></span></h1>
	<p> 
	<ul> 
	<li><p> <STRONG><a href="http://habanero-rice.github.io/hclib/" target="_blank">HClib</a></STRONG>: This is a compiler-free work-stealing library, which supports <a href="https://wiki.rice.edu/confluence/display/HABANERO/Habanero-C" target="_blank">Habanero-C</a> work-stealing constructs.  </p></li>

        <li><p> <STRONG><a href="http://habanero-rice.github.io/habanero-upc/">Habanero-UPC++</a></STRONG>: Its a compiler-free PGAS library, which supports a tighter integration of intra-place and inter-place parallelism than standard hybrid programming approaches. </p></li>

	<li><p> <STRONG><a href="http://vivkumar.github.io/ajws.html" target="_blank">AJWS</a></STRONG>: Atomic Java with work-stealing. It draws together the benefits of work-stealing and data-centric atomicity, allowing the application programmer to conveniently and succinctly expose the parallelism inherent in their Java program.  </p></li>

    	<li><p> <STRONG><a href="http://vivkumar.github.io/javatrycatchws.html" target="_blank">Java TryCatchWS</a></STRONG>: The Java try-catch work-stealing (TryCatchWS) framework is now available online. This is the software implementation of my Ph.D. research. </p></li>

	</ul>
  
	</p>
	</div>
	<br>
	<br>
	<br>


        <div class="contentText"> 
	<h1><span style="color: #9f8060; padding-top:8px; font-size:21px; font-weight:bold; letter-spacing:-1px; "><a name="services"> Professional Services </a></span></h1>
	<p>  
	<ul>
		<li><p> Program Committee Member: <a href="https://hipc.org/" target="_blank">HiPC 2019</a>, <a href="https://conf.researchr.org/home/mplr-2019" target="_blank">MPLR 2019</a>, <a href="http://respa15.rice.edu/" target="_blank">RESPA 2015</a>, <a href="http://x10-lang.org/workshop/workshop16.html" target="_blank">X10'16</a>, <a href="http://hpc.pnl.gov/IA3/IA3/Call_For_Papers.html" target="_blank">IA^3 2016</a>, <a href="http://hpc.pnl.gov/IA3/IA3/Call_For_Papers.html">IA^3 2017</a>, HiPC SRS 2017-2018</p></li>
		<li><p> Organizing Committee: <a href="http://respa15.rice.edu/" target="_blank">RESPA 2015</a> (publicity chair), <a href="http://conf.researchr.org/home/PPoPP-2017/" target="_blank">PPoPP 2017</a> (web chair)
		<li><p>	Journal Reviewer: TACO, TOPC, TPDS, TOPLAS, JPDC.</p></li>
	</ul>
	</p>
	</div>
	<br>
	<br>
	<br>

        <div class="contentText"> 
	<h1><span style="color: #9f8060; padding-top:8px; font-size:21px; font-weight:bold; letter-spacing:-1px; "><a name="photos"> Photos</a></span></h1>
	<p> My students from the courses that I have taught at IIITD:
        <ul> 
	<li>Foundations of Parallel Programming: <a href="./images/fpp_2017.jpg" target="_blank">2017</a>, <a href="./images/fpp_2018.jpg" target="_blank">2018</a>, <a href="./images/fpp_2019.jpg" target="_blank">2019</a> </li>
        <li>Advanced Programming: <a href="./images/ap_2017.jpg" target="_blank">2017</a>, <a href="./images/ap_2018.jpg" target="_blank">2018</a>, <a href="./images/ap_2019.jpg" target="_blank">2019</a> </li>
        </ul>
	</p>
	</div>
	<br>
	<br>

        <div class="contentText"> 
	<h1><span style="color: #9f8060; padding-top:8px; font-size:21px; font-weight:bold; letter-spacing:-1px; "><a name="affiliations"> Past Affiliations</a></span></h1>
	<p>  
	<img src="./logos/rice.jpg" alt="Rice" height="50" width="120" />
	<img src="./logos/anu.jpg" alt="ANU" height="50" width="120" />
	<img src="./logos/ibm.jpg" alt="IBM" height="50" width="120" />
	<br>
	<img src="./logos/geometric.png" alt="Geometric" height="50" width="120" />
	<img src="./logos/cdac.jpg" alt="CDAC" height="50" width="120" />
	<img src="./logos/sit.png" alt="SIT" height="100" width="120" />
	</p>
	</div>
	<br>
	<br>
	<br>

        <div class="contentText"> 
	<h1><span style="color: #9f8060; padding-top:8px; font-size:21px; font-weight:bold; letter-spacing:-1px; "><a name="contact"> Contact </a></span></h1>
	<p> 
	Room No. B-506, R&D Block, 5th Floor, IIIT Delhi, Near Okhla Phase 3, New Delhi-110020, India 
	<br> E-mail: vivekk @ iiitd.ac.in
	</p>
	<a href="http://www.linkedin.com/pub/vivek-kumar/21/4b1/42a" target="_blank">
        <img src="./linkedin.png" alt="Linkedin" height="20" width="70" /> </a>
	<a href="https://scholar.google.com/citations?user=cRZsY44AAAAJ&hl=en" target="_blank">
        <img src="./gscholar.png" alt="Google Scholar" height="30" width="70"/> </a>
	<a href="http://dl.acm.org/author_page.cfm?id=81548457656&coll=DL&dl=ACM&trk=0&cfid=700130054&cftoken=24930157" target="_blank">
        <img src="./acm-logo.jpg" alt="ACM DL" height="30" width="90" /> </a>
	<a href="http://dblp.uni-trier.de/pers/hd/k/Kumar_0001:Vivek" target="_blank">
        <img src="./dblp.jpeg" alt="DBLP" height="30" width="60" /> </a>
	</div>
	<br>
	<br>
	<br>
	
        <div class="contentText"> 
	<h1><span style="color: #9f8060; padding-top:8px; font-size:21px; font-weight:bold; letter-spacing:-1px; "><a name="history"> Site History </a></span></h1>
	<p> 
	<ul>
		<li><p> 
		Site created on: August 2, 2015
		</p></li>

		<li><p> 
		Previous site location: <a href="http://users.cecs.anu.edu.au/~vivek/" target="_blank">http://users.cecs.anu.edu.au/~vivek/</a> (ANU CECS department).
		</p></li>

		<li><p> 
		<script language="javascript">
                        months = ['January', 'Febraury', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'];
                        var d=new Date();
                        var d = new Date();
                        (d.getFullYear());
                        var theDate = new Date(document.lastModified);
                        with (theDate) {
                                document.write("Site last updated on:<b> "+getDate()+' '+months[getMonth()]+' '+d.getFullYear()+" CDT </b>")                        
			}      
	        </script> 
		</p></li>
	</ul>
	</p>
	</div>
	<br>
	<br>
	<br>

</body>
</html>
